---
title: "Titanic Detailed Analysis"
author: "Kushal Kapadia"
date: "October 7, 2017"
output:
  word_document: default
  html_document: default
---

```{r}
# Loading the new data
train = read.csv("train.csv", header = T)
test = read.csv("test.csv", header = T)

# Combining the dataset the old way by adding a column instead of doing bind_rows fom dplyr package
test.survived = data.frame(Survived = rep("None", nrow(test)), test[,])
data.combined = rbind(train,test.survived)
```

```{r}
# Let's have a close look at the structure of the data 
str(data.combined)

# Letting R know the categorical variables in the dataset
data.combined$Survived = as.factor(data.combined$Survived)
data.combined$Pclass = as.factor(data.combined$Pclass)

```


```{r}
# Let's have a general look at how many survived and how many did not
table(data.combined$Survived)


# Distribution across classes
table(data.combined$Pclass)

```

```{r}
# Data visualization library
library(ggplot2)

train$Pclass = as.factor(train$Pclass)
ggplot(train, aes(x = Pclass, fill = factor(Survived))) + geom_bar(width = 0.5) + xlab("Pclass") + ylab("Total Count") + labs(fill="Survived")

```

```{r}
# Examine the first few names in the training dataset
head(as.character(train$Name))

# how many unique names across both train and test dataset?
length(unique(as.character(data.combined$Name)))
#Thus, this shows us that there are two duplicate names

# Finding the two duplicate names
which(duplicated(data.combined$Name))
# One way to find the location of the two similar names
which(data.combined$Name == "Connolly, Miss. Kate")

```

```{r}
library(stringr)  #character extraction library

# Let's try to find the observations having only "Miss." using the str_detect
misses = data.combined[which(str_detect(data.combined$Name, "Miss.")), ]
misses[1:5,]

# Let's also try to find the observations having only "Mrs." using the str_detect
mrses = data.combined[which(str_detect(data.combined$Name, "Mrs.")),]
mrses[1:5,]

# check out if the pattern continues
males = data.combined[data.combined$Sex == "male",]
males[1:5,]

```

```{r}
# Now, we're gonna add variable "title" to the dataset
# Before adding the title variable, we first have to assign the title values to all the observations

# Creating a utility function to help with title extraction
extractTitle = function(name){
  name = as.character(name)
  
  if(length(grep("Miss.",name))>0){
    return("Miss.")
  } else if (length(grep("Mrs.",name))>0){
    return("Mrs.")
  } else if (length(grep("Master.",name))>0){
    return("Master.")
  } else if (length(grep("Mr.",name))>0){
    return("Mr.")
  } else {
    return("Other")
  } 
}

titles = NULL
for(i in 1:nrow(data.combined)){
  titles = c(titles, extractTitle(data.combined[i,"Name"]))
}
data.combined$title = as.factor(titles)

```


## Data Visualization ##
```{r}
par(mfrow = c(3,3))

# Before ending this, let's also try visualizing data with 3 variables together namely survived, pclass and title
ggplot(data.combined[1:891,], aes(x = title , fill = Survived)) + #remember this to write stat=count and position=dodge
  geom_bar(stat = 'count', position = "dodge") +
  facet_wrap(~Pclass) + 
  xlab("Title") + 
  ylab("Total Count") + 
  labs(fill = "Survived")

# Distribution of male to female in the combined dataset
table(data.combined$Sex)

# Now's let's visualize data a bit for pclass, sex and survived using ggplot
ggplot(data.combined[1:891,], aes(x = Sex, fill=Survived)) + 
    geom_bar(width = 0.5) + 
    facet_wrap(~Pclass) + 
    xlab("Sex") + 
    ylab("Total Counts") +
    labs(fill="Survived")

# Enough with the sex variable now. Age and sex seem to be related to each other
# Let's explore the age variable a little bit more

summary(data.combined$Age)
summary(data.combined[1:891,"Age"]) #This actually indicates that there are lot of missing values for age in the training data which is not a good thing. 
# Ways to find missing values are mean, median, mean or median of the group, imputation(basically training a model which would help us in getting the missing values) and also proxy

# Let's visualize again by using age, sex, pclass and survived. Should be interesting to code it! I'm excited!
ggplot(data.combined[1:891,], aes(x=Age, fill=Survived)) + 
  geom_histogram(binwidth = 10) + 
  facet_wrap(~Sex+Pclass) + 
  ggtitle("Sex+Pclass") + 
  xlab("Age") + 
  ylab("Total Count") + 
  labs(fill = "Survived")

# Master is a good proxy for male children. Here's why:
boys = data.combined[which(data.combined$title == "Master."),]
summary(boys$Age) #This indeed confirms that the min age is 0.33 and max age is 14.5 which means that they are male children

# Let's also delve deep into Miss title which is a bit complicated and we'll see why:
misses = data.combined[data.combined$title == "Miss.",]
summary(misses$Age) # See, actually here the min age 0.17 rises to Max 63 which is why we can't say what they exactly are. Female children or female adults

# Let's visualize something different using ggplot
ggplot(misses[misses$Survived != "None",], aes(x=Age, fill=Survived)) + 
  facet_wrap(~Pclass) +
  geom_histogram(binwidth = 5) + 
  ggtitle("Age for Miss. by Pclass") +
  xlab("Age") + 
  ylab("Total Count") 

# Okay appears that female children might have a different survival rate 
# Could be a candidate for feature engineering
misses.alone = misses[misses$SibSp == 0 & misses$Parch == 0,]
summary(misses.alone$Age)
length(which(misses.alone$Age <= 14.5))
  
# Now, let's take a look at Sibsp variable
summary(data.combined$SibSp)
table(data.combined$SibSp) #Not there in the video

# As very evident from the 2 above lines of code, we can turn Sibsp into a factor
data.combined$SibSp = as.factor(data.combined$SibSp)

# Now that we know something about Sibsp, let's try visualizing it for a wee bit:
ggplot(data.combined[1:891,], aes(x = SibSp, fill = Survived)) + 
  geom_bar(width = 0.5) + 
  facet_wrap(~Pclass + title) + 
  ggtitle("Pclass, Title") + 
  xlab("Sibsp") + 
  ylab("Total Count") +
    ylim(0,300) + 
  labs(fill = "Survived")  

# Let's have a look at parch variable which actaully means parents or something. Ain't sure enough :P
unique(data.combined$Parch)    
table(data.combined$Parch)    

# Let's go ahead and convert this also to a factor variable
data.combined$Parch = as.factor(data.combined$Parch)
    
# Same kind of visualization plot as Sibsp. Hence, Copying and  pasting it:
ggplot(data.combined[1:891,], aes(x = Parch, fill = Survived)) + 
  geom_bar(width = 0.5) + 
  facet_wrap(~Pclass + title) + 
  ggtitle("Pclass, Title") + 
  xlab("Parch") + 
  ylab("Total Count") +
ylim(0,300) + 
  labs(fill = "Survived")  

# Let's do some cool feature engineering here by creating a variable familysize
# Let's first combine Sibsp and Parch from trainig and testing dataset
temp.Sibsp = c(train$SibSp, test$SibSp)
temp.Parch = c(train$Parch, test$Parch)
data.combined$familysize = as.factor(temp.Parch + temp.Sibsp + 1)

# Visualize it to see it has some predictive power in there or not:
ggplot(data.combined[1:891,], aes(x = familysize, fill = Survived)) + 
  geom_bar(width = 0.5) + 
  facet_wrap(~Pclass + title) + 
  ggtitle("Pclass, Title") + 
  xlab("family size") + 
  ylab("Total Count") +
ylim(0,300) + 
  labs(fill = "Survived")  

```



```{r}
par(mfrow = c(3,3))

# We need to look at the ticket variable

str(data.combined$Ticket)
summary(data.combined$Ticket)

# Thus, we're gonna transform this into a character variable
data.combined$Ticket = as.character(data.combined$Ticket)
data.combined$Ticket[1:20]

# Looking at a first few values, we can extract the first string to check if something useful comes out
ticket.first.char = ifelse(data.combined$Ticket == "", " ", substr(data.combined$Ticket,1,1))
unique(ticket.first.char)
 
# Okay so we can make it a factor for analysis purposes and visualize it
data.combined$ticket.first.char = as.factor(ticket.first.char)

# First, a high level plot of data
ggplot(data.combined[1:891,], aes(x = ticket.first.char, fill = Survived)) + 
  geom_bar(width = 0.8) + 
  ylab("Total Count") + 
  xlab("Ticket.first.char") + 
  labs(fill = "Survived")

# Using Pclass
ggplot(data.combined[1:891,], aes(x = ticket.first.char, fill = Survived)) + 
  geom_bar(width = 0.8) +
  facet_wrap(~Pclass) +
  ylab("Total Count") + 
  xlab("Ticket.first.char") + 
  labs(fill = "Survived")

# Now using Pclass and Title both
  ggplot(data.combined[1:891,], aes(x = ticket.first.char, fill = Survived)) + 
    geom_bar(width = 0.8) + 
    facet_wrap(~Pclass + title) +
    ylab("Total Count") + 
    xlab("Ticket.first.char") + 
    labs(fill = "Survived")

# Next up is Fare
summary(data.combined$Fare)
str(data.combined$Fare)    #numeric variable
length(unique(data.combined$Fare))

# We can relate the fair with Pclass 
ggplot(data.combined, aes(x = Fare)) + 
  geom_histogram(binwidth = 5) + 
  xlab("Fare") + 
  ylab("Total Count") +
  ylim(0,200)

# Let's see if it has some predictive power or not
ggplot(data.combined[1:891,], aes(x = Fare, fill = Survived)) + 
  geom_histogram(binwidth = 20) +
  facet_wrap(~Pclass + title) + 
  xlab("Fare") + 
  ylab("Total Count") + 
  ggtitle("Pclass + Title") +
  labs(fill = "Survived") +
  ylim(0,20)

# Let's do something with cabin variable now
str(data.combined$Cabin) 

# Clearly it's not a factor
data.combined$Cabin = as.character(data.combined$Cabin)
data.combined$Cabin[1:100]

# Replace the missing cabins with U
data.combined[which(data.combined$Cabin == ""),"Cabin"] = "U"
data.combined$Cabin[1:100]  

# Take a look at just first character or letter
cabin.first.char = as.factor(substr(data.combined$Cabin,1,1))
str(cabin.first.char)  
levels(cabin.first.char)  

# Adding it to combined data set and then we go on to plot it to see if there's any predictive power in it or not
data.combined$cabin.first.char = cabin.first.char
```


```{r}
par(mfrow = c(3,3))

# Data Visualization again!
ggplot(data.combined[1:891,], aes(x = cabin.first.char, fill = Survived)) +
  geom_bar() + 
  xlab("Cabin.first.char") + 
  ylab("Total count")
  
#Let's drill in a bit more
ggplot(data.combined[1:891,], aes(x = cabin.first.char, fill = Survived)) +
  geom_bar() + 
  facet_wrap(~Pclass) +
  xlab("Cabin.first.char") + 
  ylab("Total count")

#Pclass + title
  ggplot(data.combined[1:891,], aes(x = cabin.first.char, fill = Survived)) +
    geom_bar() + 
    facet_wrap(~Pclass + title) +
  xlab("Cabin.first.char") + 
    ylab("Total count")
  
#What about folks with multiple cabins?
data.combined$cabin.multiple = as.factor(ifelse(str_detect(data.combined$Cabin," "), "Y", "N"))

#Onto the ggplot thing, ofcourse.
ggplot(data.combined[1:891,], aes(x = cabin.multiple, fill = Survived)) +
  geom_bar() + 
  facet_wrap(~Pclass + title)+
xlab("Cabin.first.char") + 
  ylab("Total count")
#Not particularly interesting. We shall come to it later on, maybe.

#Let's have a look at the last variable embarked
str(data.combined$Embarked)
summary(data.combined$Embarked)
levels(data.combined$Embarked)

#Some plotting again
ggplot(data.combined[1:891,], aes(x = Embarked, fill = Survived)) +
  geom_bar() + 
  facet_wrap(~Pclass + title) +
xlab("Cabin.first.char") + 
  ylab("Total count")

```



## Exploratory Data Analysis ##
```{r}
library(randomForest)

#Let's train our first random forest model using just two predictor variables Pclass and title

rf.train.1 = data.combined[1:891, c("Pclass", "title")]
rf.label = as.factor(train$Survived)

set.seed(1234)
rf.1 = randomForest(x = rf.train.1, y = rf.label, importance = T, ntree = 1000)
rf.1
varImpPlot(rf.1)

#Train a random forest model using Pclass, title and sibsp

rf.train.2 = data.combined[1:891, c("Pclass", "title", "SibSp")]

set.seed(1234)
rf.2 = randomForest(x = rf.train.2, y = rf.label, importance = T, ntree = 1000)
rf.2
varImpPlot(rf.2)


#Train a random forest model using Pclass, title and Parch

rf.train.3 = data.combined[1:891, c("Pclass", "title", "Parch")]

set.seed(1234)
rf.3 = randomForest(x = rf.train.3, y = rf.label, importance = T, ntree = 1000)
rf.3
varImpPlot(rf.3)


#Train a random forest model using Pclass, title, SibSp and Parch

rf.train.4 = data.combined[1:891, c("Pclass", "title", "Parch", "SibSp")]

set.seed(1234)
rf.4 = randomForest(x = rf.train.4, y = rf.label, importance = T, ntree = 1000)
rf.4
varImpPlot(rf.4)


#Train a random forest model using Pclass, title and familysize

rf.train.5 = data.combined[1:891, c("Pclass", "title", "familysize")]

set.seed(1234)
rf.5 = randomForest(x = rf.train.5, y = rf.label, importance = T, ntree = 1000)
rf.5
varImpPlot(rf.5)


#Train a random forest model using Pclass, title, familysize and Parch

rf.train.6 = data.combined[1:891, c("Pclass", "title", "Parch", "familysize")]

set.seed(1234)
rf.6 = randomForest(x = rf.train.6, y = rf.label, importance = T, ntree = 1000)
rf.6
varImpPlot(rf.6)


#Train a random forest model using Pclass, title, SibSp and Familysize

rf.train.7 = data.combined[1:891, c("Pclass", "title", "SibSp", "familysize")]

set.seed(1234)
rf.7 = randomForest(x = rf.train.7, y = rf.label, importance = T, ntree = 1000)
rf.7
varImpPlot(rf.7)

#As it is very evident from the above running models, the model with just title, pclass and familysize gives the highest accuracy or lowest OOB rate


```


## Cross Validation ##
```{r}
#Let's try to submit these predictions to Kaggle first and see how we're doing
test.submit.df = data.combined[892:1309, c("Pclass", "familysize", "title")]

#This is how you should predict
rf.5.preds = predict(rf.5, test.submit.df)
table(rf.5.preds)

#Write out a CSV file for the submission to Kaggle
submit.df = data.frame(PassengerId = 892:1309, Survived = rf.5.preds)

write.csv(submit.df, file = "RF1.csv", row.names = F)

#Now, as we can see from the Kaggle, our score turns out be 0.79426 but the OOB estimates predicted it to be 0.8159
#Let's dig deep into the concept of cross-validation

library(caret)
library(doSNOW)

#We're gonna be doing something called stratified cross-validation. 
set.seed(2348)
cv.10.folds = createMultiFolds(y=rf.label, k=10, times = 10)

#Check stratification
table(rf.label)
342/549

#Check for any fold now
table(rf.label[cv.10.folds[[33]]])
307/494

#For stratification, the main property is that the ratio of folks who perished to the folks who survived should be same in the each folds and the y(rf.label)

#Now, let's setup traincontrol object per above
ctrl.1 = trainControl(method = "repeatedcv", number = 10, repeats = 10, index = cv.10.folds)

#Set up doSNOW package for multi-core training. This is helpful because we're gonna be using a lot of trees
cl = makeCluster(6, type = "SOCK")
registerDoSNOW(cl)

#Set seed for reproducibility and train
set.seed(34324)
rf.5.cv.1 = train(x=rf.train.5, y=rf.label, method="rf", tuneLength=3, ntree=1000, trControl= ctrl.1)

#Shut down cluster
stopCluster(cl)

#Check out results
rf.5.cv.1



#Let's also try 5-fold to see if there is improvment in accuracy
set.seed(5983)
cv.5.folds = createMultiFolds(y=rf.label, k=5, times = 10)

ctrl.2 = trainControl(method = "repeatedcv", number = 5, repeats = 10, index = cv.5.folds)

#Set up doSNOW package for multi-core training. This is helpful because we're gonna be using a lot of trees
cl = makeCluster(6, type = "SOCK")
registerDoSNOW(cl)

#Set seed for reproducibility and train
set.seed(89472)
rf.5.cv.2 = train(x=rf.train.5, y=rf.label, method="rf", tuneLength=3, ntree=1000, trControl= ctrl.2)

#Shut down cluster
stopCluster(cl)

#Check out results
rf.5.cv.2



#3-fold
set.seed(5986)
cv.3.folds = createMultiFolds(y=rf.label, k=3, times = 10)

ctrl.3 = trainControl(method = "repeatedcv", number = 3, repeats = 10, index = cv.3.folds)

#Set up doSNOW package for multi-core training. This is helpful because we're gonna be using a lot of trees
cl = makeCluster(6, type = "SOCK")
registerDoSNOW(cl)

#Set seed for reproducibility and train
set.seed(89465)
rf.5.cv.3 = train(x=rf.train.5, y=rf.label, method="rf", tuneLength=3, ntree=1000, trControl= ctrl.3)

#Shut down cluster
stopCluster(cl)

#Check out results
rf.5.cv.3

```


```{r}
#Let's see where we might have gone wrong. Let's build a single decision tree to check what exactly is happening on the inside
#Random forests are ofcourse way better than decision trees but when it comes to easily understand the whole picture, deciion trees
#are way better than random forests

library(rpart)
#install.packages("rpart.plot")
library(rpart.plot)

#Create utility function
rpart.cv = function(seed, training, labels, ctrl) {
  cl = makeCluster(6, type = "SOCK")
  registerDoSNOW(cl)
  
  set.seed(seed)
  #Leverage formula interface for training
  rpart.cv = train(x=training, y=labels, method="rpart", tuneLength=30, trControl = ctrl)
  
  #shutdown cluster
  stopCluster(cl)
  
  return(rpart.cv)
}

#Grab features
features = c("Pclass", "title", "familysize")
rpart.train.1 = data.combined[1:891, features]

#Run CV and check out results
rpart.1.cv.1 = rpart.cv(94622, rpart.train.1, rf.label, ctrl.3)
rpart.1.cv.1

#Plot
prp(rpart.1.cv.1$finalModel, type = 0, extra = 1, under = T)


#The plot brings out some interesting lines of investigation. Namely:
#     1 - Titles of Mr. and Others are predicted to perish at an overall accuracy of 83.2%
#     2 - Titles of Master, Miss and Mrs. in 1st and 2nd class are predicted to survive at 
#         an overall accuracy of 94.9%
#     3 - Titles of Master, Miss & Mrs in class 3 and having a family size of 5,6,8,11
#         are predicted to perish at an overall accuracy of 100%
#     4 - Titles of Master, Miss & Mrs in class 3 and having a family size of 5,6,8,11
#         are predicted to survive at an overall accuracy of 59.6%

#Both rpart and ef confirm that title is important. Let's investigate further:
#Also, we're stressing more on the 1st point here that the title Mr and other just seem blunt. 
#Let's move ahead and investigate it further
table(data.combined$title)

#Parse out last name and title
data.combined$Name[1:5]

name.splits = str_split(data.combined$Name, ",")
name.splits[1]
last.names = sapply(name.splits, "[", 1)
last.names[1:10]

#Add last names to the data.combined in case we might find it useful later
data.combined$last.name = last.names

#Now for titles
name.splits = str_split(sapply(name.splits,"[",2)," ")
titles = sapply(name.splits,"[",2)
unique(titles)

#What's up with the title of "the"?
which(titles == "the")
data.combined[760,]

#Re-map titles to be more exact
titles[titles %in% c("the", "Dona.")] = "Lady."
titles[titles %in% c("Ms.", "Mlle.")] = "Miss."
titles[titles == "Mme."] = "Mrs."
titles[titles %in% c("Jonkheer", "Don.")] = "Sir."
titles[titles %in% c("Col.", "Capt.", "Major.")] = "Officer"
table(titles)


#Now add this to the dataframe again
data.combined$new.title = as.factor(titles)

#Let's again use this for data visualization
ggplot(data.combined[1:891,], aes(x=new.title, fill=Survived)) +
  geom_bar(width = 0.5) +
  facet_wrap(~Pclass) +
  xlab("New title") +
  ylab("Counts") +
  labs(fill = "Survived")
  
#Collapse titles based on visual analysis
indexes = which(data.combined$new.title == "Lady.")
data.combined$new.title[indexes] = "Mrs."

indexes = which(data.combined$new.title == "Dr." |
                  data.combined$new.title == "Rev." |
                  data.combined$new.title == "Sir." |
                  data.combined$new.title == "Officer")
data.combined$new.title[indexes] = "Mr."

#Visualize again
ggplot(data.combined[1:891,], aes(x=new.title, fill=Survived)) +
  geom_bar(width = 0.5) +
  facet_wrap(~Pclass) +
  xlab("New title") +
  ylab("Counts") +
  labs(fill = "Survived")

#Grab features
features = c("Pclass", "new.title", "familysize")
rpart.train.2 = data.combined[1:891, features]

```


```{r}
#Run CV and check out results
rpart.2.cv.1 = rpart.cv(94622, rpart.train.2, rf.label, ctrl.3)
rpart.2.cv.1

#Plot
prp(rpart.2.cv.1$finalModel, type = 0, extra = 1, under = T)


#Dive in on 1st class Mr.
indexes.first.mr = which(data.combined$new.title == "Mr." & data.combined$Pclass == "1")
first.mr.df = data.combined[indexes.first.mr,]
summary(first.mr.df)

#1 female?
first.mr.df[first.mr.df$Sex == "female",]
#Here, we can see that she is Dr. and has been classified as a "Mr."

#Let's update new.title feature
indexes = which(data.combined$Sex == "female" & data.combined$new.title == "Mr.")
data.combined$new.title[indexes] = "Mrs."

#Any other gender slip-ups?
length(which(data.combined$Sex == "female" & (data.combined$new.title == "Master." | data.combined$new.title == "Mr.")))

#Refresh dataframe 
indexes.first.mr = which(data.combined$new.title == "Mr." & data.combined$Pclass == "1")
first.mr.df = data.combined[indexes.first.mr,]

#Let's look at surviving 1st class "Mr."
summary(first.mr.df[first.mr.df$Survived == "1",])
View(first.mr.df[first.mr.df$Survived == "1",])

#Take a look at some of the high fares
indexes = which(data.combined$Ticket == "PC 17755" |
                data.combined$Ticket ==  "113760" |
                data.combined$Ticket == "PC 17611")
View(data.combined[indexes,])

#Visualize survival rates for 1st class "Mr." by fare
ggplot(first.mr.df, aes(x = Fare, fill = Survived)) + 
  geom_density(alpha = 0.5) +
  ggtitle("1st class Mr. survival rate by fare")

#Engineer features based on all the passengers with the same ticket 
ticket.party.size = rep(0, nrow(data.combined))
avg.fare = rep(0.0, nrow(data.combined) )
tickets = unique(data.combined$Ticket)

for(i in 1:length(tickets)) {
  current.ticket = tickets[i]
  party.indexes = which(data.combined$Ticket == current.ticket)
  current.avg.fare = data.combined[party.indexes[1], "Fare"] / length(party.indexes)
  
  for(k in i:length(party.indexes)){
    ticket.party.size[party.indexes[k]] = length(party.indexes)
    avg.fare[party.indexes[k]] = current.avg.fare
  }
}

data.combined$ticket.party.size = ticket.party.size
data.combined$avg.fare = avg.fare
data.combined$`avg. fare` = NULL

#Refresh 1st class "Mr." dataframe
first.mr.df = data.combined[indexes.first.mr,]
summary(first.mr.df)

#Visualize new features 
ggplot(first.mr.df[first.mr.df$Survived != "None",], aes(x = ticket.party.size, fill=Survived)) + 
  geom_density(alpha = 0.5) + 
  ggtitle("Survival rates 1st class Mr. by ticket.party.size")

ggplot(first.mr.df[first.mr.df$Survived != "None",], aes(x = avg.fare, fill=Survived)) + 
  geom_density(alpha = 0.5) + 
  ggtitle("Survival rates 1st class Mr. by ticket.party.size")

#Hypothesis - ticket.party.size is highly correlated with avg. fare
summary(data.combined$avg.fare)

#Let's figure out the NA value
which(is.na(data.combined$avg.fare))
data.combined[1044,]

#Get records for similar passengers and summarize avg. fares 
indexes = with(data.combined, which(Pclass == "3" & title == "Mr." & familysize == "1" & Ticket != "3701"))
similar.na.passengers = data.combined[indexes,]
summary(similar.na.passengers$avg.fare)

#Use median since it is very close to mean and slightly higher than mean
data.combined[is.na(avg.fare),"avg.fare"] = 7.840

#Leverage caret's preProcess function to normalize data
prepoc.data.combined = data.combined[,c("ticket.party.size", "avg.fare")]
prePoc = preProcess(prepoc.data.combined, method = c("center", "scale"))

postproc.data.combined = predict(prePoc, prepoc.data.combined)

#Let's check the correlation between avg.fare and ticket.party.size
cor(postproc.data.combined$ticket.party.size, postproc.data.combined$avg.fare)
#Correlation always results between -1 and 1 where -1 is negatively correlated and +1 means completely correlated. 0 means no correlation at all
#Here, they are highly uncorrelated means we have two new potential features that we could add

#How about just 1st class all up?
indexes = which(data.combined$Pclass == "1")
cor(postproc.data.combined$ticket.party.size[indexes], postproc.data.combined$avg.fare[indexes])
#Hypothesis refuted again

#Okay. Let's see if our feature engineering has made any difference or not
features = c("Pclass", "new.title","familysize", "ticket.party.size", "avg.fare")
rpart.train.3 = data.combined[1:891,features]

#Run CV and check out results
rpart.3.cv.1 = rpart.cv(94622, rpart.train.3, rf.label, ctrl.3)
rpart.3.cv.1

#Plot
prp(rpart.3.cv.1$finalModel, type = 0, extra = 1, under = T)


######################################################
######## Submitting, scoring and some analysis #######
######################################################

#Rpart

#Subset our test records and features
test.submit.df = data.combined[892:1309, features]

#Make predictions
rpart.3.preds = predict(rpart.3.cv.1$finalModel, test.submit.df, type = "class")
table(rpart.3.preds)

#Write out a CSV file for submission to kaggle
submit.df = data.frame(PassengerId = 892:1309, Survived = rpart.3.preds)

write.csv(submit.df, file = "Rpart2.csv", row.names = F)



# random forest

features = c("Pclass", "new.title","familysize", "ticket.party.size", "avg.fare")
rf.train.temp = data.combined[1:891,features]

set.seed(1234)
rf.temp = randomForest(x = rf.train.temp, y = rf.label, ntree = 1000)
rf.temp

test.submit.df = data.combined[892:1309, features]

# Make predictions 
rf.preds = predict(rf.temp, test.submit.df)
table(rf.preds)

# Write out a CSV file
submit.df = data.frame(PassengerId = 892:1309, Survived = rf.preds)

write.csv(submit.df, file = "RF2.csv", row.names = F)

```







